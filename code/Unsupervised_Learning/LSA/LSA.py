from sklearn.decomposition import TruncatedSVD

data = [
    [1, 0, 0, 0],
    [1, 0, 0, 0],
    [1, 1, 0, 0],
    [0, 1, 0, 0],
    [0, 0, 1, 1],
    [0, 0, 1, 0],
    [0, 0, 1, 1],
    [0, 0, 0, 1]
]
n_components = 2
model = TruncatedSVD(n_components=n_components)
model.fit(data)
# 変換データ
print(model.transform(data))
# 寄与率
print(model.explained_variance_ratio_)
# 累積寄与率
print(sum(model.explained_variance_ratio_))  # 0.67　２個の変数で元データの67％が説明できている

"""
[[ 0.00000000e+00  8.50650808e-01]
 [ 0.00000000e+00  8.50650808e-01]
 [-2.71947991e-16  1.37638192e+00]
 [-2.71947991e-16  5.25731112e-01]
 [ 1.41421356e+00  2.02192262e-16]
 [ 7.07106781e-01  1.01096131e-16]
 [ 1.41421356e+00  2.02192262e-16]
 [ 7.07106781e-01  1.01096131e-16]]
[0.38596491 0.27999429]
0.6659592065833292
"""
"""
大量の文書から関連性を探索する。同義性問題。単語と単語の類似度、単語と文書の類似度。潜在的な意味空間への変換。（これによって似ている単語とコンピュータに教えることができる）
特異値分解。

"""